{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f142bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MMU\\Downloads\\FYP_Prototype\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\MMU\\Downloads\\FYP_Prototype\\venv\\lib\\site-packages\\malaya\\tokenizer.py:214: FutureWarning: Possible nested set at position 3397\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n",
      "c:\\Users\\MMU\\Downloads\\FYP_Prototype\\venv\\lib\\site-packages\\malaya\\tokenizer.py:214: FutureWarning: Possible nested set at position 3927\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n"
     ]
    }
   ],
   "source": [
    "# ✅ SOSNet Cleaning, Translation & Augmentation Pipeline\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from slangdict import slangdict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import malaya\n",
    "from malaya.augmentation import abstractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c535fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Load SOSNet files ===\n",
    "sosnet_dir = \"C:/Users/MMU/Downloads/FYP_Prototype/FYP_Prototype/model/IEEE Big Data 2020 Cyberbullying Dataset\"\n",
    "file_map = {\n",
    "    \"8000age.txt\": \"age\",\n",
    "    \"8000ethnicity.txt\": \"ethnicity\",\n",
    "    \"8000gender.txt\": \"gender\",\n",
    "    \"8000religion.txt\": \"religion\",\n",
    "    \"8000other.txt\": \"other_cyberbullying\",\n",
    "    \"8000notcb.txt\": \"not_cyberbullying\"\n",
    "}\n",
    "data = []\n",
    "for fname, label in file_map.items():\n",
    "    with open(os.path.join(sosnet_dir, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "        tweets = f.read().splitlines()\n",
    "        data.extend([(tweet.strip(), label) for tweet in tweets if tweet.strip()])\n",
    "df = pd.DataFrame(data, columns=[\"tweet_text\", \"cyberbullying_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d344b09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MMU\\Downloads\\FYP_Prototype\\venv\\lib\\site-packages\\ekphrasis\\classes\\tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MMU\\Downloads\\FYP_Prototype\\venv\\lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "✅ BEFORE Filtering:\n",
      " cyberbullying_type\n",
      "other_cyberbullying    8006\n",
      "gender                 8004\n",
      "not_cyberbullying      8001\n",
      "ethnicity              8000\n",
      "religion               8000\n",
      "age                    7999\n",
      "Name: count, dtype: int64\n",
      "✅ AFTER Filtering:\n",
      " cyberbullying_type\n",
      "religion               7999\n",
      "age                    7996\n",
      "ethnicity              7996\n",
      "gender                 7931\n",
      "not_cyberbullying      7772\n",
      "other_cyberbullying    7592\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def normalize_slang(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([slangdict.get(w, w) for w in words])\n",
    "\n",
    "def is_low_quality(text):\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return True\n",
    "    if len(text.split()) < 3:\n",
    "        return True\n",
    "    if len(re.findall(r'[a-zA-Z0-9]', text)) / max(len(text), 1) < 0.3:\n",
    "        return True\n",
    "    if not re.search(r'[a-zA-Z]', text):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'],\n",
    "    annotate=set(),\n",
    "    fix_html=True,\n",
    "    segmenter=\"twitter\",\n",
    "    corrector=\"twitter\",\n",
    "    unpack_hashtags=True,\n",
    "    unpack_contractions=True,\n",
    "    spell_correct_elong=False,\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "\n",
    "df[\"normalized_text\"] = df[\"tweet_text\"].apply(lambda t: normalize_slang(t.lower()))\n",
    "df[\"is_low_quality\"] = df[\"normalized_text\"].apply(is_low_quality)\n",
    "df_filtered = df[df[\"is_low_quality\"] == False].copy()\n",
    "\n",
    "print(\"✅ BEFORE Filtering:\\n\", df[\"cyberbullying_type\"].value_counts())\n",
    "print(\"✅ AFTER Filtering:\\n\", df_filtered[\"cyberbullying_type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24a21f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # make sure this is imported\n",
    "\n",
    "model_name = \"mesolitica/translation-t5-base-standard-bahasa-cased\"\n",
    "trans_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "trans_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trans_model = trans_model.to(device)\n",
    "\n",
    "def split_text(text, chunk_size=100):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks, current = [], \"\"\n",
    "    for s in sentences:\n",
    "        if len(current) + len(s) <= chunk_size:\n",
    "            current += \" \" + s\n",
    "        else:\n",
    "            chunks.append(current.strip())\n",
    "            current = s\n",
    "    if current: chunks.append(current.strip())\n",
    "    return chunks\n",
    "\n",
    "def safe_translate_text(text, model, tokenizer, max_tokens=256, batch_size=64):\n",
    "    try:\n",
    "        chunks = split_text(str(text))\n",
    "        if not chunks:\n",
    "            return \"\"\n",
    "        translated_chunks = []\n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch = chunks[i:i + batch_size]\n",
    "            inputs = tokenizer([f\"terjemah ke Melayu: {chunk}\" for chunk in batch],\n",
    "                               return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            inputs.pop(\"token_type_ids\", None)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=max_tokens, no_repeat_ngram_size=2)\n",
    "            translated_chunks.extend([tokenizer.decode(t, skip_special_tokens=True) for t in outputs])\n",
    "        return \" \".join(translated_chunks)\n",
    "    except Exception as e:\n",
    "        print(\"[Translation Error]\", str(e)[:100], \"| Text:\", text[:80])\n",
    "        return \"\"\n",
    "\n",
    "def fast_batch_translate(texts, model, tokenizer, max_tokens=256, batch_size=128):\n",
    "    try:\n",
    "        inputs = tokenizer([f\"terjemah ke Melayu: {t}\" for t in texts],\n",
    "                           return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        inputs.pop(\"token_type_ids\", None)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=max_tokens, no_repeat_ngram_size=2)\n",
    "        return [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
    "    except Exception as e:\n",
    "        print(\"[Batch Translation Error]\", str(e))\n",
    "        return [\"\" for _ in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5381fbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔸 Original:   \"Two things that I just can't respect.... A fuck nigga and a lyin ass bitch\" @YoungDolph DOLPHHHHH!!!\n",
      "🧽 Cleaned:    \" two things that i just can not respect . . . . a fuck nigga and a lyin ass bitch \" <user> dolphhhhh ! ! !\n",
      "🌐 Translated: Dua perkara yang saya tidak boleh hormati.. A fucknigga dan seorang lipin pantat bitch <user> Dolphhhhh!\n",
      "--------------------------------------------------------------------------------\n",
      "🔸 Original:   If a white man called me, a black woman, a negro, I would be offended. I would consider it a slur.\n",
      "🧽 Cleaned:    if a white man called me , a black woman , a negro , i would be offended . i would consider it a slur .\n",
      "🌐 Translated: Jika seorang lelaki kulit putih memanggil saya, wanita berkulit hitam, Saya akan menganggapnya sebagai omong kosong.\n",
      "--------------------------------------------------------------------------------\n",
      "🔸 Original:   @y_alibhai you smear and insult Jews and cravenly cheer on the faction of left wing useful idiots who hate Jews more than they adore Palestinians. Question. Will YOU be asking British Muslims about Islamists HAMAS ISIS HIZBALLAH AL QEIDA TALEBAN who all share the same ideology\n",
      "🧽 Cleaned:    <user> you smear and insult jews and cravenly cheer on the faction of left wing useful idiots who hate jews more than they adore palestinians . question . will you be asking british muslims about islamists hamas isis hizballah al qeida taleban who all share the same ideology\n",
      "🌐 Translated:  Anda memfitnah dan menghina Yahudi serta dengan penuh semangat menyokong puak sayap kiri yang berguna bodoh kerana lebih membenci orang Israel daripada mereka memuja rakyat Palestin. soalan. Adakah anda akan bertanya kepada Muslim British tentang Islamis seperti Hamas, ISIS,Hizballah,Al-Qeida,Alikahban,dan mereka yang berkongsi ideologi dengan orang lain?\n",
      "--------------------------------------------------------------------------------\n",
      "🔸 Original:   How did libs go from: - Oh God, I guess Creepy Joe To - Women candidates! To - Gay candidate! To - Stop Bernie! That fucking shit bag sexist communist loving nazi trump 2.0 To - Yay Rape! #BlueNoMatterWho #BlueMAGA #BlueWave2020 Thus: pic.twitter.com/LshOE2vyhs\n",
      "🧽 Cleaned:    how did libs go from : - oh god , i guess creepy joe to - women candidates ! to - gay candidate ! to - stop bernie ! that fucking shit bag sexist communist loving nazi trump <number> to - yay rape ! blue no matter who blue maga blue wave 2020 thus : pic . twitter . com / lshoe2vyhs\n",
      "🌐 Translated: Bagaimana libs berubah dari: Oh Tuhan, saya rasa Joe yang menyeramkan kepada calon wanita! Kepada Calon gay! Berhenti bernie! Beg yang sangat buruk itu seksis, komunis penyayang Nazi Trump <nombor> kepada - yay perkosaan! biru tidak kira siapa biru, Maga Biru Blue Wave 2020 oleh itu: gambar. twitter.com/lshoe2vyhs\n",
      "--------------------------------------------------------------------------------\n",
      "🔸 Original:   You 2 dumb ugly fuck nigger name wayne &amp; ugly ass chris black brown positive positive your not red ugly ass wayne pic.twitter.com/Gco24jHrCN\n",
      "🧽 Cleaned:    you <number> dumb ugly fuck nigger name wayne & ugly ass chris black brown positive positive your not red ugly ass wayne pic . twitter . com / gco24jhrcn\n",
      "🌐 Translated:  Anda mempunyai nombor yang tidak menarik, tetapi anda kelihatan seperti nama nigger dan pantat hodoh. Chris adalah seorang lelaki kulit hitam dengan warna coklat positif dalam gambar Wayne bukan merah atau punggung ugly. twitter .com / pgco24jhrcn\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 🔍 Preview 10 translations\n",
    "test_df = df_filtered.sample(5, random_state=42)\n",
    "for orig in test_df[\"tweet_text\"]:\n",
    "    cleaned = \" \".join(text_processor.pre_process_doc(orig))\n",
    "    translated = safe_translate_text(cleaned, trans_model, trans_tokenizer)\n",
    "    print(f\"🔸 Original:   {orig}\")\n",
    "    print(f\"🧽 Cleaned:    {cleaned}\")\n",
    "    print(f\"🌐 Translated: {translated}\\n{'-' * 80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ddf47a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Resuming from checkpoint_12000.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Look for the latest checkpoint\n",
    "checkpoint_files = sorted(\n",
    "    [f for f in os.listdir() if f.startswith(\"checkpoint_\") and f.endswith(\".csv\")],\n",
    "    key=lambda x: int(re.findall(r'\\d+', x)[0])\n",
    ")\n",
    "\n",
    "if checkpoint_files:\n",
    "    latest_checkpoint = checkpoint_files[-1]\n",
    "    print(f\"🔄 Resuming from {latest_checkpoint}\")\n",
    "    \n",
    "    df_checkpoint = pd.read_csv(latest_checkpoint)\n",
    "    translated_texts = df_checkpoint[\"translated\"].tolist()\n",
    "    cleaned_texts = df_checkpoint[\"cleaned\"].tolist()\n",
    "    processed_count = len(translated_texts)\n",
    "else:\n",
    "    print(\"🆕 No checkpoint found. Starting fresh.\")\n",
    "    translated_texts, cleaned_texts = [], []\n",
    "    processed_count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6aa5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checkpoint.to_csv(f\"checkpoint_{len(translated_texts)}.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9266a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Translating Tweets:  34%|███▍      | 16000/47286 [43:22<5:04:35,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint saved at 4000 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating Tweets:  42%|████▏     | 20004/47286 [1:23:39<4:23:26,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint saved at 8000 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating Tweets:  51%|█████     | 23968/47286 [2:01:23<3:52:15,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint saved at 12000 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating Tweets:  59%|█████▉    | 28000/47286 [2:46:25<3:30:29,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint saved at 16000 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating Tweets:  68%|██████▊   | 32000/47286 [3:34:28<1:44:58,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint saved at 20000 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating Tweets:  76%|███████▌  | 36000/47286 [3:51:58<36:36,  5.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint saved at 24000 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating Tweets:  85%|████████▍ | 39968/47286 [4:12:36<1:00:18,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint saved at 28000 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating Tweets:  93%|█████████▎| 43968/47286 [4:31:53<23:26,  2.36it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint saved at 32000 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating Tweets: 100%|██████████| 47286/47286 [4:47:53<00:00,  2.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Prepare lists\n",
    "translated_texts, cleaned_texts = [], []\n",
    "batch, cleaned_batch = [], []\n",
    "\n",
    "# Language detection\n",
    "lang_model = malaya.language_detection.fasttext(\n",
    "    model='mesolitica/fasttext-language-detection-ms-id'\n",
    ")\n",
    "\n",
    "# 🔹 Stage 1: Translate all tweets first\n",
    "batch_count = 0\n",
    "checkpoint_interval = 2000  # every 2000 tweets\n",
    "\n",
    "for tweet in tqdm(df_filtered[\"tweet_text\"].iloc[processed_count:], desc=\"Translating Tweets\", initial=processed_count, total=len(df_filtered)):\n",
    "    cleaned = \" \".join(text_processor.pre_process_doc(tweet))\n",
    "    cleaned_batch.append(cleaned)\n",
    "    batch.append(tweet)\n",
    "\n",
    "    if len(batch) == 32:\n",
    "        short_texts = [t for t in cleaned_batch if len(t.split()) < 100]\n",
    "        long_texts = [t for t in cleaned_batch if len(t.split()) >= 100]\n",
    "\n",
    "        fast_trans = fast_batch_translate(short_texts, trans_model, trans_tokenizer)\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            slow_trans = list(executor.map(safe_translate_with_fallback, long_texts))\n",
    "\n",
    "        translated_texts.extend(fast_trans + slow_trans)\n",
    "        cleaned_texts.extend(short_texts + long_texts)\n",
    "\n",
    "        batch_count += 1\n",
    "\n",
    "        # 🛟 Auto-save every 2000 translated tweets\n",
    "        if len(translated_texts) % checkpoint_interval == 0:\n",
    "            df_checkpoint = pd.DataFrame({\n",
    "                \"cleaned\": cleaned_texts,\n",
    "                \"translated\": translated_texts\n",
    "            })\n",
    "            df_checkpoint.to_csv(f\"checkpoint_{len(translated_texts)}.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"💾 Checkpoint saved at {len(translated_texts)} tweets\")\n",
    "\n",
    "        batch, cleaned_batch = [], []\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Final flush\n",
    "if batch:  \n",
    "    short_texts = [t for t in cleaned_batch if len(t.split()) < 100]\n",
    "    long_texts = [t for t in cleaned_batch if len(t.split()) >= 100]\n",
    "\n",
    "    fast_trans = fast_batch_translate(short_texts, trans_model, trans_tokenizer)\n",
    "    slow_trans = [safe_translate_text(t, trans_model, trans_tokenizer) for t in long_texts]\n",
    "\n",
    "    translated_texts.extend(fast_trans + slow_trans)\n",
    "    cleaned_texts.extend(short_texts + long_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "549be8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved translated output to translated_malay_full.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_translated = pd.DataFrame({\n",
    "    \"cleaned_english\": cleaned_texts,\n",
    "    \"translated_malay\": translated_texts,\n",
    "    \"cyberbullying_type\": df_filtered[\"cyberbullying_type\"].tolist()[processed_count:]\n",
    "})\n",
    "\n",
    "df_translated.to_csv(\"clean_translated_full_v2.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"✅ Saved translated output to translated_malay_full.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# ✅ Load original filtered dataframe\n",
    "df_full = df_filtered.copy()\n",
    "\n",
    "# ✅ Step 1: Decide how many tweets are missing\n",
    "missing_start = 0\n",
    "missing_end = 12000\n",
    "\n",
    "# ✅ Step 2: Select only missing tweets\n",
    "df_missing = df_full.iloc[missing_start:missing_end]\n",
    "\n",
    "# ✅ Step 3: Translate missing tweets\n",
    "translated_texts = []\n",
    "cleaned_texts = []\n",
    "batch, cleaned_batch = [], []\n",
    "\n",
    "def safe_translate_with_fallback(text):\n",
    "    try:\n",
    "        return safe_translate_text(text, trans_model, trans_tokenizer)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] {e} | Text: {text[:80]}\")\n",
    "        return \"\"\n",
    "\n",
    "def fast_batch_translate(texts, model, tokenizer, max_tokens=256, batch_size=64):\n",
    "    translated = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        prompts = [f\"terjemah ke Melayu: {t}\" for t in batch]\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items() if k != \"token_type_ids\"}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=max_tokens, no_repeat_ngram_size=2)\n",
    "        translated.extend([tokenizer.decode(o, skip_special_tokens=True) for o in outputs])\n",
    "    return translated\n",
    "\n",
    "# ✅ Step 4: Translate\n",
    "for tweet in tqdm(df_missing[\"tweet_text\"], desc=\"Translating Missing Tweets\"):\n",
    "    cleaned = \" \".join(text_processor.pre_process_doc(tweet))\n",
    "    cleaned_batch.append(cleaned)\n",
    "    batch.append(tweet)\n",
    "\n",
    "    if len(batch) == 32:\n",
    "        short_texts = [t for t in cleaned_batch if len(t.split()) < 100]\n",
    "        long_texts = [t for t in cleaned_batch if len(t.split()) >= 100]\n",
    "\n",
    "        fast_trans = fast_batch_translate(short_texts, trans_model, trans_tokenizer)\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            slow_trans = list(executor.map(safe_translate_with_fallback, long_texts))\n",
    "\n",
    "        translated_texts.extend(fast_trans + slow_trans)\n",
    "        cleaned_texts.extend(short_texts + long_texts)\n",
    "\n",
    "        batch, cleaned_batch = [], []\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ✅ Final small batch if still left\n",
    "if batch:\n",
    "    short_texts = [t for t in cleaned_batch if len(t.split()) < 100]\n",
    "    long_texts = [t for t in cleaned_batch if len(t.split()) >= 100]\n",
    "\n",
    "    fast_trans = fast_batch_translate(short_texts, trans_model, trans_tokenizer)\n",
    "    slow_trans = [safe_translate_text(t, trans_model, trans_tokenizer) for t in long_texts]\n",
    "\n",
    "    translated_texts.extend(fast_trans + slow_trans)\n",
    "    cleaned_texts.extend(short_texts + long_texts)\n",
    "\n",
    "# ✅ Step 5: Save missing 1–12000 into DIFFERENT file\n",
    "df_missing_translated = pd.DataFrame({\n",
    "    \"cleaned\": cleaned_texts,\n",
    "    \"translated\": translated_texts,\n",
    "    \"cyberbullying_type\": df_missing[\"cyberbullying_type\"].values   # ⬅️ Add label here\n",
    "})\n",
    "\n",
    "\n",
    "df_missing_translated.to_csv(\"translated_missing_1_to_12000.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"✅ Saved missing translations: translated_missing_1_to_12000.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0239f34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved recovered file with labels: translated_missing_1_to_12000_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "# Reload saved CSV\n",
    "df_translated_only = pd.read_csv(\"translated_missing_1_to_12000.csv\")\n",
    "\n",
    "# Get original labels\n",
    "df_labels_only = df_filtered.iloc[0:len(df_translated_only)][\"cyberbullying_type\"].reset_index(drop=True)\n",
    "\n",
    "# Combine back properly\n",
    "df_fixed = pd.DataFrame({\n",
    "    \"cleaned_english\": df_translated_only[\"cleaned_english\"],\n",
    "    \"translated_malay\": df_translated_only[\"translated_malay\"],\n",
    "    \"cyberbullying_type\": df_labels_only\n",
    "})\n",
    "\n",
    "# Save fixed version\n",
    "df_fixed.to_csv(\"translated_missing_1_to_12000_fixed.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ Saved recovered file with labels: translated_missing_1_to_12000_fixed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c94263a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully merged! Saved to clean_translated_combined.csv\n"
     ]
    }
   ],
   "source": [
    "# Load missing translation\n",
    "df_missing = pd.read_csv(\"translated_missing_1_to_12000_fixed.csv\")\n",
    "\n",
    "# Load existing translation\n",
    "df_existing = pd.read_csv(\"clean_translated_full_v2.csv\")\n",
    "\n",
    "# ✅ Optional but recommended: ensure same columns\n",
    "assert list(df_existing.columns) == list(df_missing.columns), \"❗ Column mismatch detected!\"\n",
    "\n",
    "# ✅ Merge (missing first, then existing)\n",
    "df_combined = pd.concat([df_missing, df_existing], ignore_index=True)\n",
    "\n",
    "# ✅ Save\n",
    "df_combined.to_csv(\"clean_translated_combined.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"✅ Successfully merged! Saved to clean_translated_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47e2759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Cyberbullying Type Distribution:\n",
      "\n",
      "cyberbullying_type\n",
      "religion               7999\n",
      "age                    7996\n",
      "ethnicity              7996\n",
      "gender                 7931\n",
      "not_cyberbullying      7772\n",
      "other_cyberbullying    7592\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load your merged file\n",
    "df = pd.read_csv(\"merged_translated_all_v2.csv\")\n",
    "\n",
    "# Show class distribution\n",
    "print(\"📊 Cyberbullying Type Distribution:\\n\")\n",
    "print(df[\"cyberbullying_type\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b000f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selective Augmentation: 100%|██████████| 47286/47286 [1:29:50<00:00,  8.77it/s]  \n"
     ]
    }
   ],
   "source": [
    "# 🔹 Load merged dataset\n",
    "df = pd.read_csv(\"merged_translated_all_v2.csv\")\n",
    "translated_texts = df[\"translated_malay\"].tolist()\n",
    "original_labels = df[\"cyberbullying_type\"].tolist()\n",
    "\n",
    "# 🔹 Load Malaya augmenter\n",
    "augmenter = abstractive.huggingface(\n",
    "    model='mesolitica/translation-nanot5-small-malaysian-cased'\n",
    ")\n",
    "\n",
    "# 🔹 Define augmentation rules\n",
    "augmentation_prob = {\n",
    "    \"gender\": 0.3,               # 🔹 Mild boost\n",
    "    \"not_cyberbullying\": 0.5,    # 🔸 Bigger boost for low recall\n",
    "    \"other_cyberbullying\": 0.7   # 🔸 Largest boost to fix false positives\n",
    "}\n",
    "\n",
    "# 🔹 Run selective augmentation\n",
    "augmented_texts = []\n",
    "augmented_flags = []\n",
    "\n",
    "for text, label in tqdm(zip(translated_texts, original_labels), total=len(df), desc=\"Selective Augmentation\"):\n",
    "    try:\n",
    "        lang = lang_model.predict(text)[0].lower()\n",
    "        prob = augmentation_prob.get(label, 0)\n",
    "\n",
    "        if lang in [\"standard-malay\", \"local-malay\", \"malay\"] and random.random() < prob:\n",
    "            result = augmenter.generate([text], to_lang=\"pasar ms\", max_length=128)[0]\n",
    "            if isinstance(result, list):\n",
    "                augmented_texts.append(result[0])\n",
    "            else:\n",
    "                augmented_texts.append(result)\n",
    "            augmented_flags.append(True)\n",
    "        else:\n",
    "            augmented_texts.append(None)\n",
    "            augmented_flags.append(False)\n",
    "    except Exception as e:\n",
    "        print(\"[Aug Error]\", e)\n",
    "        augmented_texts.append(None)\n",
    "        augmented_flags.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7360d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 Combine back into the original DataFrame\n",
    "df[\"augmented_malay\"] = augmented_texts\n",
    "df[\"was_augmented\"] = augmented_flags\n",
    "df[\"final_text\"] = df[\"augmented_malay\"].combine_first(df[\"translated_malay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "603555f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Augmentation done.\n",
      "cyberbullying_type\n",
      "religion               7999\n",
      "age                    7996\n",
      "ethnicity              7996\n",
      "gender                 7931\n",
      "not_cyberbullying      7772\n",
      "other_cyberbullying    7592\n",
      "Name: count, dtype: int64\n",
      "cyberbullying_type\n",
      "other_cyberbullying    2396\n",
      "not_cyberbullying      1925\n",
      "gender                 1411\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 🔍 Optional: Show class distribution of augmented samples\n",
    "print(\"\\n✅ Augmentation done.\")\n",
    "print(df[\"cyberbullying_type\"].value_counts())\n",
    "print(df[df[\"was_augmented\"] == True][\"cyberbullying_type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eddf8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 Save updated version\n",
    "df.to_csv(\"augmented_merged_translated_all_v3.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b67729af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Language breakdown per class:\n",
      "source               augmented  english  translated\n",
      "cyberbullying_type                                 \n",
      "age                          0     7996        7996\n",
      "ethnicity                    0     7996        7996\n",
      "gender                    1411     7931        7931\n",
      "not_cyberbullying         1925     7772        7772\n",
      "other_cyberbullying       2396     7592        7592\n",
      "religion                     0     7999        7999\n"
     ]
    }
   ],
   "source": [
    "# Load full DataFrame with all sources\n",
    "df = pd.read_csv(\"augmented_merged_translated_all_v3.csv\")\n",
    "\n",
    "# Tag each row with source based on where the text is available\n",
    "records = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    label = row[\"cyberbullying_type\"]\n",
    "\n",
    "    if pd.notna(row.get(\"cleaned_english\")):\n",
    "        records.append({\"source\": \"english\", \"cyberbullying_type\": label})\n",
    "    if pd.notna(row.get(\"translated_malay\")):\n",
    "        records.append({\"source\": \"translated\", \"cyberbullying_type\": label})\n",
    "    if pd.notna(row.get(\"augmented_malay\")):\n",
    "        records.append({\"source\": \"augmented\", \"cyberbullying_type\": label})\n",
    "\n",
    "# Create DataFrame\n",
    "df_lang_dist = pd.DataFrame(records)\n",
    "\n",
    "# Group and count\n",
    "lang_summary = df_lang_dist.groupby([\"cyberbullying_type\", \"source\"]).size().unstack(fill_value=0)\n",
    "\n",
    "print(\"📊 Language breakdown per class:\")\n",
    "print(lang_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0fc61fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final class distribution:\n",
      "cyberbullying_type\n",
      "gender                 12000\n",
      "other_cyberbullying    12000\n",
      "not_cyberbullying      12000\n",
      "religion               11999\n",
      "age                    11994\n",
      "ethnicity              11994\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"augmented_merged_translated_all_v3.csv\")\n",
    "\n",
    "final_rows = []\n",
    "\n",
    "# Target total per class\n",
    "target_total = 12000\n",
    "\n",
    "for label in df[\"cyberbullying_type\"].unique():\n",
    "    df_label = df[df[\"cyberbullying_type\"] == label]\n",
    "\n",
    "    # 🔹 Separate sources\n",
    "    eng_rows = df_label[df_label[\"cleaned_english\"].notna()]\n",
    "    trans_rows = df_label[df_label[\"translated_malay\"].notna()]\n",
    "    aug_rows = df_label[df_label[\"augmented_malay\"].notna()]\n",
    "\n",
    "    # 1️⃣ Take all augmented\n",
    "    selected_aug = aug_rows\n",
    "\n",
    "    # 2️⃣ Take half English\n",
    "    selected_eng = eng_rows.sample(frac=0.5, random_state=42)\n",
    "\n",
    "    # 3️⃣ Remaining needed\n",
    "    remaining = target_total - len(selected_aug) - len(selected_eng)\n",
    "\n",
    "    # 4️⃣ Take from translated (only how many available)\n",
    "    if remaining > len(trans_rows):\n",
    "        selected_trans = trans_rows  # take all available\n",
    "    else:\n",
    "        selected_trans = trans_rows.sample(n=remaining, random_state=42)\n",
    "\n",
    "\n",
    "    # 🧩 Combine selected parts\n",
    "    final_rows.extend([selected_aug, selected_eng, selected_trans])\n",
    "\n",
    "# 🧹 Merge all\n",
    "df_final = pd.concat(final_rows, ignore_index=True)\n",
    "\n",
    "# 🛠️ Merge text columns into single 'tweet_text'\n",
    "def combine_text(row):\n",
    "    if pd.notna(row.get(\"augmented_malay\")):\n",
    "        return row[\"augmented_malay\"]\n",
    "    elif pd.notna(row.get(\"translated_malay\")):\n",
    "        return row[\"translated_malay\"]\n",
    "    elif pd.notna(row.get(\"cleaned_english\")):\n",
    "        return row[\"cleaned_english\"]\n",
    "    return None\n",
    "\n",
    "df_final[\"tweet_text\"] = df_final.apply(combine_text, axis=1)\n",
    "\n",
    "# Keep only necessary columns\n",
    "df_final = df_final[[\"tweet_text\", \"cyberbullying_type\"]]\n",
    "\n",
    "# Show final counts\n",
    "print(\"✅ Final class distribution:\")\n",
    "print(df_final[\"cyberbullying_type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "913bd89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"final_balanced_dataset_v3.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c28efc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
